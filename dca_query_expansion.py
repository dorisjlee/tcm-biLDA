#!/usr/bin/python
# -*- coding: utf-8 -*-
### Author: Edward Huang

import argparse
from multiprocessing import Pool
import numpy as np
import operator
import os
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import sys
import time

### This script rewrites the test files, except with query expansion performed
### on each query patient's list of symptoms. Query expansion is done by
### word embeddings.

def get_dca_similarity_code_list():
    '''
    Returns the mappings for the columns and rows in the similarity matrix.
    '''
    similarity_code_list = []
    f = open('./data/herb_symptom_dictionary.txt', 'r')
    f.readline() # Skip the header line.
    for line in f:
        line = line.strip().split('\t')

        line_length = len(line)
        # Some symptoms don't have good English translations.
        assert line_length == 2 or line_length == 5
        herb, symptom = line[:2]
        if herb not in similarity_code_list:
            similarity_code_list += [herb]
        if symptom not in similarity_code_list:
            similarity_code_list += [symptom]
    f.close()
    return similarity_code_list

def read_dca_similarity_dct(similarity_code_list, df_fname):
    '''
    Returns a dictionary of similarity scores.
    Key: (code_a, code_b) -> tuple(str, str)
    Value: cosine similarity -> float
    '''
    similarity_dct = {}
    f = open('./data/similarity_matrix.txt', 'r')
    for i, line in enumerate(f):
        index_a, index_b, score = line.split()
        similarity_dct[(int(index_a), int(index_b))] = abs(float(score))
    f.close()
    # Unstack the dictionary to create a dataframe.
    similarity_df = pd.Series(similarity_dct).unstack()
    # Name the rows and columns.
    similarity_df.index = similarity_code_list
    similarity_df.columns = similarity_code_list
    # Write the dataframe out to CSV.
    similarity_df.to_csv(df_fname)

def read_prosnet_matrix(run_num):
    '''
    Creates the similarity matrix generated by prosnet.
    '''
    symptom_herb_list = get_entities('symptom', run_num) + get_entities('herb',
        run_num)
    num_dim = 50
    f = open('./data/prosnet_data/prosnet_node_vectors_%d_dims_%s.vec' % (
        num_dim, run_num))
    node_list, vector_matrix = [], []
    for i, line in enumerate(f):
        if i == 0:
            continue
        line = line.split()
        node = line[0]
        # Skip codes that aren't in the expansion list.
        if node not in symptom_herb_list:
            continue
        node_list += [node]
        vector_matrix += [map(float, line[1:])]
    f.close()
    # TODO: absolute value or not?
    # similarity_matrix = np.abs(cosine_similarity(np.array(vector_matrix)))
    similarity_matrix = cosine_similarity(np.array(vector_matrix))

    similarity_dct = {}
    for a, node_a in enumerate(node_list):
        for b, node_b in enumerate(node_list):
            if a == b:
                continue
            similarity_dct[(node_a, node_b)] = similarity_matrix[a][b]
    return node_list, similarity_dct

def get_entities(code_type, run_num):
    entity_list = []
    f = open('./data/count_dictionaries/%s_count_dct_%d.txt' % (code_type,
        run_num), 'r')
    for line in f:
        entity_list += [line.split()[0]]
    f.close()
    return entity_list

def get_expansion_terms(symptom_list, similarity_df, similarity_code_list,
    training_code_list):
    '''
    Given a query list, find 10 terms that have the highest similarity scores
    to the symptoms in symptom_list.
    '''
    candidate_term_dct = {}
    for query_symptom in symptom_list:
        # Skip a query if it isn't in the dictionary.
        if query_symptom not in similarity_code_list:
            continue
        for training_code in training_code_list:
            # Skip candidates that are already in the query.
            if training_code in symptom_list:
                continue
            # Skip candidates that aren't in the dictionary.
            if training_code not in similarity_code_list:
                continue

            score = similarity_df[query_symptom][training_code]
            # Keep only terms that have a score above a threshold.
            if args.embed_method == 'dca' and score < args.sim_thresh:
                continue
            elif args.embed_method == 'prosnet' and score < 0.3:
                continue
            candidate_term_dct[training_code] = score
    # Get the top 10 terms.
    expansion_terms = sorted(candidate_term_dct.items(),
        key=operator.itemgetter(1), reverse=True)
    # Extract just the terms from the sorted list.
    expansion_terms = [term[0] for term in expansion_terms]
    return expansion_terms

def query_expansion(run_num, similarity_df, similarity_code_list):
    '''
    Runs the query expansion.
    '''
    # The list of medical codes in the training set.
    if args.term_type == 'symptoms':
        # training_code_list = get_count_dct('symptom', run_num).keys()[:]
        training_code_list = get_entities('symptom', run_num)
    elif args.term_type == 'herbs':
        training_code_list = get_entities('herb', run_num)
    elif args.term_type == 'mixed':
        training_code_list = get_entities('symptom', run_num) + get_entities(
            'herb', run_num)
        
    # Process output filename.
    out_fname = './data/train_test/test_%s_%g_%s_expansion_%d.txt' % (
        args.embed_method, args.sim_thresh, args.term_type, run_num)

    out = open(out_fname, 'w')
    f = open('./data/train_test/test_no_expansion_%d.txt' % run_num, 'r')
    for query in f:
        # Split by tab, fifth element, split by comma, take out trailing comma.
        query = query.split('\t')
        symptom_list = query[4].split(':')
        # # TODO: Not expanding on patients that have at least 5 symptoms.
        # if len(symptom_list) >= 10:
        #     out.write('\t'.join(query))
        #     continue

        expansion_terms = get_expansion_terms(symptom_list, similarity_df,
            similarity_code_list, training_code_list)

        # # TODO: fill expansion terms up to 10 terms?
        # expansion_terms = expansion_terms[:10-len(symptom_list)]
        # expansion_terms = expansion_terms[:5]

        # Write expanded query to file
        expanded_query = query[:]
        if expansion_terms != []:
            expanded_query[4] += ':' + ':'.join(expansion_terms)
        
        out.write('\t'.join(expanded_query))
    f.close()
    out.close()

def parse_args():
    global args
    parser = argparse.ArgumentParser()
    parser.add_argument('-m', '--embed_method', choices=['dca', 'prosnet'],
        required=True, help='Type of query expansion to test.')
    parser.add_argument('-s', '--sim_thresh', required=True,
        help='Type of rank metric to use.', type=float)
    parser.add_argument('-t', '--term_type', choices=['herbs', 'symptoms',
        'mixed'], required=True, help='Type of query expansion terms.')
    args = parser.parse_args()

def main():
    parse_args()

    # The keys will become the mappings for the similarity matrix.
    if args.embed_method == 'dca':
        df_fname = './data/similarity_df.csv'
        similarity_code_list = get_dca_similarity_code_list()
        # Write out the dataframe if we haven't already.
        if not os.path.exists(df_fname):
            read_dca_similarity_dct(similarity_code_list, df_fname)
        similarity_df = pd.read_csv(df_fname, index_col=0)
        # Make sure similarity dataframe is correct.
        num_codes = len(similarity_code_list)
        assert similarity_df.shape == (num_codes, num_codes)

    # query_expansion(0, similarity_df, similarity_code_list)
    pool = Pool(processes=10)
    for run_num in range(10):
        # There's a ProSNet network for each fold.
        # TODO: similarity_dct to similarity_df?
        if args.embed_method == 'prosnet':
            similarity_code_list, similarity_dct = read_prosnet_matrix(run_num)
        pool.apply_async(query_expansion, (run_num, similarity_df,
            similarity_code_list))
    pool.close()
    pool.join()

if __name__ == '__main__':
    main()