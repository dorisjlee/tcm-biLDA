{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given Query (test set data), find top-k co-occurring words for expanding the query.\n",
    "from glob import glob\n",
    "from monolingual_lda_baseline import *\n",
    "import pandas as pd\n",
    "# patient_dct = get_patient_dct(filename)\n",
    "# for test_file in glob('data/train_test/test_no_expansion_*.txt'):\n",
    "for runIdx in range(1): #range(10):\n",
    "    test_file  = 'data/train_test/test_no_expansion_{}.txt'.format(runIdx)\n",
    "    word_distr = np.loadtxt('./results/lda_word_distribution_train_no_expansion_{}.txt'.format(runIdx))\n",
    "    symptoms_keys=list(pd.read_csv(\"./data/symptom_count_dct_train_no_expansion_{}.txt\".format(runIdx),delimiter='\\t',header=None)[0])    \n",
    "    with open(test_file) as tf:\n",
    "        queries = tf.readlines()\n",
    "        for query in queries:\n",
    "            #Queries are symptoms\n",
    "            symptoms =  query.split('\\t')[4].split(':')\n",
    "#             print symptoms_lst     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for s in symptoms_keys:\n",
    "#     print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\xe8\\x88\\x8c\\xe5\\xb9\\xb2\\xe6\\xb6\\xa9',\n",
       " '\\xe8\\x83\\x83\\xe8\\x84\\x98\\xe7\\x95\\x8f\\xe5\\xaf\\x92\\xe6\\x98\\x8e\\xe6\\x98\\xbe\\xe5\\xa5\\xbd\\xe8\\xbd\\xac',\n",
       " '\\xe8\\x88\\x8c\\xe6\\x9a\\x97\\xe7\\xba\\xa2\\xe8\\x83\\x96',\n",
       " '\\xe5\\x8f\\xa3\\xe8\\x87\\xad',\n",
       " '\\xe8\\x88\\x8c\\xe7\\x81\\xbc\\xe7\\x83\\xad\\xe6\\x84\\x9f',\n",
       " '\\xe5\\xa4\\xa7\\xe4\\xbe\\xbf\\xe5\\x81\\x8f\\xe5\\xb9\\xb2',\n",
       " '\\xe5\\x97\\xb3\\xe6\\xb0\\x94\\xe5\\xa5\\xbd\\xe8\\xbd\\xac',\n",
       " '\\xe8\\x84\\x89\\xe6\\xb2\\x89\\xe7\\xbb\\x86',\n",
       " '\\xe8\\x88\\x8c\\xe6\\x9c\\xa8',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_term_prob = np.zeros_like(word_distr[:,0])\n",
    "sIdx_lst = []\n",
    "for s in symptoms:\n",
    "    try:\n",
    "        if s!=\"\":\n",
    "            sIdx = symptoms_keys.index(s)\n",
    "            sIdx_lst.append(sIdx)\n",
    "            query_term_prob += word_distr[:,sIdx]\n",
    "    except(ValueError):\n",
    "        print \"Ignore: testing symptom not in training keys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_k_topic_query_match = argsort(query_term_prob)[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 40, 53, 62, 63, 26, 87, 24, 57, 35])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_topic_query_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find what additional symptoms do these topics have in common, i.e. which words other than sIdx ones have high prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 1644, 1646, ..., 2058,  723, 1662],\n",
       "       [   0, 1640, 1641, ..., 2277,  586,  826],\n",
       "       [   0, 1649, 1650, ...,  775, 1501, 1638],\n",
       "       ..., \n",
       "       [   0, 1641, 1642, ..., 1404,  202, 1744],\n",
       "       [   0, 1639, 1640, ..., 1183, 2205, 1820],\n",
       "       [   0, 1641, 1642, ...,  261,  290, 1212]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(word_distr[top_k_topic_query_match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "舌干涩\n",
      "胃脘畏寒明显好转\n",
      "舌暗红胖\n",
      "口臭\n",
      "舌灼热感\n",
      "大便偏干\n",
      "嗳气好转\n",
      "脉沉细\n",
      "舌木\n"
     ]
    }
   ],
   "source": [
    "for sIdx in sIdx_lst:\n",
    "    print symptoms_keys[sIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_expansion_terms=5\n",
    "expansion_Idx_lst = np.argsort(word_distr[top_k_topic_query_match])[::-1][:,num_expansion_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\xe5\\xb7\\xa6\\xe4\\xb8\\x8b\\xe8\\x85\\xb9\\xe7\\x96\\xbc\\xe7\\x97\\x9b',\n",
       "       '\\xe8\\x88\\x8c\\xe6\\xb7\\xa1\\xe7\\xb4\\xab\\xe6\\x9a\\x97',\n",
       "       '\\xe8\\x89\\xb2\\xe7\\x99\\xbd',\n",
       "       '\\xe5\\xb0\\xbf\\xe4\\xb8\\x8d\\xe5\\xb0\\xbd\\xe6\\x84\\x9f',\n",
       "       '\\xe5\\xb0\\xbf\\xe4\\xb8\\x8d\\xe5\\xb0\\xbd\\xe6\\x84\\x9f',\n",
       "       '\\xe8\\x82\\x9b\\xe9\\x97\\xa8\\xe4\\xb8\\x8b\\xe5\\x9d\\xa0\\xe6\\x84\\x9f',\n",
       "       'CSG', '\\xe5\\xa4\\x9c\\xe5\\xb0\\xbf\\xe9\\xa2\\x91',\n",
       "       '\\xe8\\x88\\x8c\\xe6\\xb7\\xa1\\xe7\\xb4\\xab\\xe6\\x9a\\x97',\n",
       "       '\\xe4\\xb8\\xa4\\xe8\\x83\\x81\\xe4\\xb8\\x8d\\xe9\\x80\\x82'], \n",
       "      dtype='|S39')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "左下腹疼痛\n",
      "舌淡紫暗\n",
      "色白\n",
      "尿不尽感\n",
      "尿不尽感\n",
      "肛门下坠感\n",
      "CSG\n",
      "夜尿频\n",
      "舌淡紫暗\n",
      "两胁不适\n"
     ]
    }
   ],
   "source": [
    "for expansion_Idx in expansion_Idx_lst:\n",
    "    print symptoms_keys[expansion_Idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\xe5\\xb7\\xa6\\xe4\\xb8\\x8b\\xe8\\x85\\xb9\\xe7\\x96\\xbc\\xe7\\x97\\x9b'\n",
      " '\\xe8\\x88\\x8c\\xe6\\xb7\\xa1\\xe7\\xb4\\xab\\xe6\\x9a\\x97'\n",
      " '\\xe8\\x89\\xb2\\xe7\\x99\\xbd'\n",
      " '\\xe5\\xb0\\xbf\\xe4\\xb8\\x8d\\xe5\\xb0\\xbd\\xe6\\x84\\x9f'\n",
      " '\\xe5\\xb0\\xbf\\xe4\\xb8\\x8d\\xe5\\xb0\\xbd\\xe6\\x84\\x9f'\n",
      " '\\xe8\\x82\\x9b\\xe9\\x97\\xa8\\xe4\\xb8\\x8b\\xe5\\x9d\\xa0\\xe6\\x84\\x9f' 'CSG'\n",
      " '\\xe5\\xa4\\x9c\\xe5\\xb0\\xbf\\xe9\\xa2\\x91'\n",
      " '\\xe8\\x88\\x8c\\xe6\\xb7\\xa1\\xe7\\xb4\\xab\\xe6\\x9a\\x97'\n",
      " '\\xe4\\xb8\\xa4\\xe8\\x83\\x81\\xe4\\xb8\\x8d\\xe9\\x80\\x82']\n"
     ]
    }
   ],
   "source": [
    "print np.array(symptoms_keys)[expansion_Idx_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手脚凉\n"
     ]
    }
   ],
   "source": [
    "print symptoms_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe5\\xb7\\xa6\\xe4\\xb8\\x8b\\xe8\\x85\\xb9\\xe7\\x96\\xbc\\xe7\\x97\\x9b:\\xe4\\xb8\\xa4\\xe8\\x83\\x81\\xe4\\xb8\\x8d\\xe9\\x80\\x82:\\xe5\\xa4\\xa7\\xe4\\xbe\\xbf\\xe6\\x9c\\x89\\xe6\\x97\\xb6\\xe7\\xa8\\x80:\\xe8\\xaf\\xb8\\xe7\\x97\\x87\\xe5\\x8a\\xb3\\xe7\\xb4\\xaf\\xe5\\x8a\\xa0\\xe9\\x87\\x8d:\\xe8\\x89\\xb2\\xe7\\x99\\xbd:\\xe8\\x88\\x8c\\xe6\\xb7\\xa1\\xe7\\xb4\\xab\\xe6\\x9a\\x97:\\xe8\\xaf\\xb8\\xe7\\x97\\x87\\xe5\\x8a\\xb3\\xe7\\xb4\\xaf\\xe5\\x8a\\xa0\\xe9\\x87\\x8d:\\xe8\\x85\\xb9\\xe7\\x97\\x9b:CSG:\\xe5\\xb0\\xbf\\xe4\\xb8\\x8d\\xe5\\xb0\\xbd\\xe6\\x84\\x9f'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\":\".join(expansion_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expanded_query = query.split('\\t')\n",
    "expanded_query[4] +=\":\".join(expansion_terms)\n",
    "print '\\t'.join(expanded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe7\\x83\\xa7\\xe5\\xbf\\x83'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.split(':')[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runIdx =1\n",
    "# for runIdx in range(10):\n",
    "test_file  = 'data/train_test/test_no_expansion_{}.txt'.format(runIdx)\n",
    "word_distr = np.loadtxt('./results/lda_word_distribution_train_no_expansion_{}.txt'.format(runIdx))\n",
    "symptoms_keys=list(pd.read_csv(\"./data/symptom_count_dct_train_no_expansion_{}.txt\".format(runIdx),delimiter='\\t',header=None)[0])    \n",
    "test_lda_expansion_file =open('data/train_test/test_lda_expansion_{}.txt'.format(runIdx),'w')\n",
    "\n",
    "with open(test_file) as tf:\n",
    "    queries = tf.readlines()\n",
    "    for query in queries:\n",
    "        #Queries are symptoms\n",
    "#         print query\n",
    "        symptoms =  query.split('\\t')[4].split(':')\n",
    "#             print symptoms_lst     \n",
    "        # Populate a matrix of word prob only for the words that's inside the symptoms query\n",
    "        query_term_prob = np.zeros_like(word_distr[:,0])\n",
    "        sIdx_lst = []\n",
    "        for s in symptoms:\n",
    "            try:\n",
    "#                 print \"working\"\n",
    "                if s!=\"\":\n",
    "                    sIdx = symptoms_keys.index(s)\n",
    "                    sIdx_lst.append(sIdx)\n",
    "                    query_term_prob += word_distr[:,sIdx]\n",
    "            except(ValueError):\n",
    "#                 print \"Ignore: testing symptom not in training keys\"\n",
    "                pass\n",
    "        # Pick the top k topics that contains a lot of the query terms\n",
    "        top_k_topic_query_match = argsort(query_term_prob)[::-1][:10]\n",
    "        # Find what additional symptoms do these topics have in common, i.e. which words other than sIdx ones have high prob\n",
    "        num_expansion_terms=5\n",
    "        expansion_Idx_lst = np.argsort(word_distr[top_k_topic_query_match])[::-1][:,num_expansion_terms]\n",
    "#         print expansion_Idx_lst\n",
    "        expansion_terms = np.array(symptoms_keys)[expansion_Idx_lst]\n",
    "        #Write expanded query to file\n",
    "        expanded_query = query.split('\\t')\n",
    "        expanded_query[4] +=\":\".join(expansion_terms)\n",
    "        test_lda_expansion_file.write('\\t'.join(expanded_query)+'\\n')\n",
    "test_lda_expansion_file.close()\n",
    "#         print \"-------\"\n",
    "#         for terms in expansion_terms:\n",
    "#             print terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
